---
title: '<center> <h1>Machine Learning (M0-163)</h1> </center>'
author: "María Plaza García"
subtitle: '`r params$subtitulo`'
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
citecolor: blue
urlcolor: blue
output:
  pdf_document: 
    fig_caption: yes
    highlight: tango
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    fig_caption: yes
    toc_float: true
    theme: united
    highlight: tango
    toc_depth: 3
  word_document:
    toc: true
link-citations: yes
nocite: |
  @lantz2015machine
  @max2017caret
  @core2013r
  @kuhn2017caret
header-includes:
  - \usepackage[spanish]{babel}
  - \usepackage{subfig}
params:
  file: BreastCancer1.csv
  folder.data: ./datos
  subtitulo: Tercera prueba de evaluación continua - Cáncer de mama
  p.train: !r 2/3
  k_value: !r c(1,5,11,17,23,27)
  seed.train: 12345
  seed.clsfier: 1234567
bibliography: PEC3.bib
geometry: margin=2cm
---

```{r setup, include=FALSE}
require(knitr)
knitr::opts_chunk$set(
  fig.show = "hold",
  fig.align='center',
  fig.height = 4.5,
  fig.width = 5,
	message = FALSE,
	warning = FALSE,
	cache = TRUE,
	comment = NULL,
	tidy = FALSE
)
options(width=90)
Sys.setlocale("LC_TIME", "C")
```

```{r packages, message=FALSE, warning=FALSE, include=FALSE}
usePackage <- function(p) {    
    if (!is.element(p, installed.packages()[,1]))
        install.packages(p, dep = TRUE)
    require(p, character.only = TRUE)
}
usePackage("caret")
usePackage("kableExtra")
usePackage("gmodels")
usePackage("pROC")
usePackage("kernlab")
usePackage("randomForest")
usePackage("knitr")
usePackage("C50")
usePackage("class")
```

**Repositorio Github:**

https://github.com/mariaplaza/PEC3_MachineLearning

\pagebreak

# Introducción

En esta PEC vamos a resolver un análisis para detectar el cáncer en las mediciones de células biopsiadas de mujeres con masas mamarias anormales.

Utilizaremos datos de cáncer de mama que incluyen mediciones de imágenes digitalizadas de aspiración con aguja fina de una masa mamaria. Los valores representan características de los núcleos celulares presentes en la imagen digital.

En el fichero BreastCancer1.csv estan los datos sobre el cáncer de mama de 569 casos de biopsias de cáncer, cada uno con 32 características. La primera característica es un número de identificación, después son 30 mediciones de laboratorio con valores numéricos y por último, esta el diagnóstico. El diagnóstico se codifica como M para indicar maligno o B para indicar benigno.

Los ficheros necesarios para realizar la PEC estan en formato csv. Se encuentran dentro de mi repositorio github [@github2016github], asi como cada uno de los archivos creados para la realización de esta PEC:

https://github.com/mariaplaza/PEC3_Machine_Learning

# Objetivo

En esta PEC se analizan estos datos mediante la implementación de los diferentes algoritmos estudiados: k-Nearest Neighbour, Naive Bayes, Artificial Neural Network, Support Vector Machine, Árbol de Decisión y Random Forest para diagnosticar el tipo de cáncer de mama. Para ello se ha empleado la función de cada algoritmo como se presenta en @lantz2015machine y el paquete `caret` en alguno de los casos.

# Trabajando con los datos
## Lectura de datos

Para facilitar la reproducibilidad del informe, se han incluido varios parámetros en el encabezado `YAML` del documento cuyos valores se pueden establecer cuando se procesa el informe. Se ha incluido tanto la semilla que emplearemos más tarde en la creación de los datos de test y de entrenamiento así como los nombres de los archivos y la ruta de acceso, de esta forma podemos leer los datos con el siguiente código:

```{r datos, message=FALSE, warning=FALSE}
# Ahora ya se importan los datos a formato data.frame
library(readr)
m.file <- ifelse(params$folder.data=="", 
                 params$file,
                 file.path(params$folder.data,params$file))
data <- read.csv(file=m.file)
```

## Exploración y preparación de los datos

Comprobamos que el dataset está completo y se presentan los seis primeros registros de las 6 primeras variables:

```{r}
dim(data)
head(data)[,1:6]
```

Nuestro conjunto de datos, *`r params$file`*, esta formado por `r nrow(data)` registros, con valores de `r ncol(data)` características.

La primera variable es una variable numérica denominada `id`. Ya que es un simple identificador de cada paciente en los datos, no proporciona infromación útil y necesitamos excluirlo del modelo.
Como está localizado en la primera columna, podemos excluirlo con el siguiente código, eliminando del *data.frame* la columna número 1.

```{r}
# eliminamos la característica id
data <- data[-1]
```

La última variable, "diagnosis", es de particular interés, ya que es el resultado que esperamos predecir. Esta característica indica si la muestra es de tipo benigno "B" o maligno "M". La salida `table ()` indica que las muestras `r as.numeric(table(data$diagnosis))["B"]` son benignas mientras
que `r as.numeric(table(data$diagnosis))["M"]` son malignas. La clasificación existente según la diagnosis:

```{r}
# tabla de diagnóstico, según la tipología 
table(data$diagnosis)
```

La mayoría de los clasificadores en *Machine Learning* requiren que esta característica objetivo esté codificada como factor, por lo que recodificamos la variable, de tipo caracter, en tipo factor.

```{r}
class(data$diagnosis)

data$diagnosis <- factor(data$diagnosis, levels = c("B", "M"),
                         labels = c("Benigno", "Maligno"))

class(data$diagnosis)
```

Ahora, cuando miramos la salida `prop.table ()`, notamos que los valores han sido etiquetados como 'Benigno' y 'Maligno'.

```{r}
# tabla de la proporción de cada tipo
round(prop.table(table(data$diagnosis)) * 100, digits = 1)
```

Las características restantes de `r ncol(data) -31` son todas numéricas y, como era de esperar, consisten en mediciones diferentes de 30 características. Por ejemplo:

```{r}
# incluimos tres características numéricas
kable(summary(data[c("radius_mean", "area_mean", "smoothness_mean")]))
```

La estructura final es:
```{r}
str(data)
```

Veamos como se comportan las características estudiadas según la clasificación de nuestra principal variable, disgnosis. Realizamos para ello una exploración gráfica con un boxplot de las variables numéricas. Como veremos, se pueden observarlas diferencias en la escala de estas variables, y para poder distinguir adecuadamente las variables, debemos cambiar el límite de los ejes:

```{r fig.width=8}
boxplot(data,main='Datos sin normalizar',col=c('lightgreen', 'orange'),cex.axis=0.7,
        subset=data$diagnosis, ylim=c(0.05,1000))

boxplot(data,main='Datos sin normalizar',col=c('lightgreen', 'orange'),cex.axis=0.7,
        subset=data$diagnosis, ylim=c(0,0.30))
```

Además se presenta el comportamiento de las primeras 12 variables del dataset en función del tipo de diagnóstico:

```{r echo=FALSE, fig.height=6, fig.width=7.5}
Conf4x3 = matrix(c(1:12), nrow=3, byrow=TRUE)
layout(Conf4x3)

boxplot(data[,1] ~ data$diagnosis, ylab = "radius_mean", xlab = "", col=c("lightgreen",
                                                                          "orange"))
boxplot(data[,2] ~ data$diagnosis, ylab = "texture_mean", xlab = "", col=c("lightgreen", 
                                                                           "orange"))
boxplot(data[,3] ~ data$diagnosis, ylab = "perimeter_mean", xlab = "", col=c("lightgreen", 
                                                                             "orange"))
boxplot(data[,4] ~ data$diagnosis, ylab = "area_mean", xlab = "", col=c("lightgreen", 
                                                                        "orange"))
boxplot(data[,5] ~ data$diagnosis, ylab = "smoothness_mean", xlab = "", col=c("lightgreen", 
                                                                              "orange"))
boxplot(data[,6] ~ data$diagnosis, ylab = "compactness_mean", xlab = "", col=c("lightgreen",
                                                                               "orange"))
boxplot(data[,7] ~ data$diagnosis, ylab = "concavity_mean", xlab = "", col=c("lightgreen", 
                                                                             "orange"))
boxplot(data[,8] ~ data$diagnosis, ylab = "concave.points_mean", xlab = "", col=c("lightgreen",
                                                                                  "orange"))
boxplot(data[,9] ~ data$diagnosis, ylab = "symmetry_mean", col=c("lightgreen", 
                                                                 "orange"))
boxplot(data[,10] ~ data$diagnosis, ylab = "fractal_dimension_mean", xlab = "",col=c("lightgreen", 
                                                                                      "orange"))
boxplot(data[,11] ~ data$diagnosis, ylab = "radius_se", xlab = "", col=c("lightgreen", 
                                                                         "orange"))
boxplot(data[,12] ~ data$diagnosis, ylab = "texture_se", xlab = "", col=c("lightgreen", 
                                                                          "orange"))
```

Cada variable muestra diferencias en los resultados según pertenezcan a tejido sano o tumoral. Sin embargo, aunque normalmente el tejido sano parece mostrar valores menores que el tejido tumoral, depende generalmente de la variable. Mostrando por ejemplo valores muy similares en la viariable "fractal_dimension_mean". Podemos ver cada variable de forma independiente, pero ya que tenemos un gran número de variables, el estudio puede extenderse demasiado. 

Como muestra, podemos realizar un anáisis aleatorio de todas las muestras. Para ello, se toma una muestra de 15 tejidos sanos y otra muestra de tejidos tumorales, analizando la distribución resultante de 5 variabls aleatorias:

```{r fig.height=4, fig.width=6}
indicesT <- which(data$diagnosis =='Maligno')
indicesN <- which(data$diagnosis =='Benigno')

set.seed(123456)
aleatT <- sample(indicesT,15)
set.seed(params$seed.train)
aleatN <- sample(indicesN,15)

#10 muestras aleatorias
set.seed(123456)
aleatG <- sample(1:(ncol(data)-1),5)

#Dataset "reducidos" con las muestras
datasetT <- data[aleatT,aleatG]
datasetN <- data[aleatN,aleatG]

par(mfrow=c(1,2))
boxplot(datasetN,cex.axis=0.6,ylab='',main="Tejido normal",las=2, col="lightgreen")
abline(h = 500,col='red')
boxplot(datasetT,cex.axis=0.6,ylab='', main="Tejido tumoral", las=2, col = "orange")
abline(h = 500,col='red')
```

Por otro lado, podemos realizar una normalización de los datos, de forma que todos se encuentran entre 0 y 1 y nos permita una visualización  más sencilla y fácil de interpretar:

```{r}
# Función de normalización
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```

Ahora podemos aplicar la función `normalize()` a las características numéricas en nuestro marco de datos. En lugar de normalizar cada una de las 30 variables numéricas individualmente, utilizaremos una de las funciones de R para automatizar el proceso.

```{r fig.width=8}
# normalizamos los datos
data.norm <- as.data.frame(lapply(data[,-31], normalize))
# Los graficamos
boxplot(data.norm,main='Datos normalizados',col=c('lightblue', "orange"),cex.axis=0.7,
        subset=data$diagnosis)
abline(h=0.5,lwd=1, col="red")
```

## Partición de los datos en training/test

Realizamos *aleatoriamente* una extracción de los datos para entrenar el modelo, en concreto  `r round(params$p.train *100,2)`% de todas las observaciones, que son `r round(params$p.train*nrow(data),0)` registros,  y el resto, `r (nrow(data) - round(params$p.train*nrow(data),0))` registros, para evaluarlo (test). Usamos los datos sin normalizar y si en algún caso necesitan normalización o alguna otra transformación lo haremos directamente dentro del análisis de cada modelo.

```{r }
#fijamos la semilla para el generador pseudoaleatorio
set.seed(params$seed.train) 
# creamos los grupos de entrenamiento (datos.train) y prueba (datos.test) con la selección 
# aleatoria de los datos asignando el 67% a train y 33% a test (train).
train<-sample(1:nrow(data),round(nrow(data)*params$p.train,0))
datos.train <- data.frame(data[train,])
datos.test  <- data.frame(data[-train,])

nrow(datos.train)/nrow(datos.test) # debe estar alrededor de 2
```

El porcentaje de muestras con el diagnóstico de maligno con nuestros datos de entrenamiento es:

```{r}
prop.table(table(datos.train$diagnosis))
```

El porcentaje de muestras con el diagnóstico de maligno con nuestros datos de test es:
```{r}
prop.table(table(datos.test$diagnosis))
```

Vemos que es muy similar, por lo que los datos están equilibrados.

# Aplicación de cada uno de los algoritmos para la clasificación
## k-Nearest Neighbour
### Transformación de los datos

Para aplicar el algoritmo de k-nearest necesitamos tener los datos normalizados. Ya que se debe aplicar la misma selección de datos training y test en todos los algoritmos, empleamos la misma selección de datos anteriores (dentro del vector *train*) pero sobre los datos normalizados y creamos de nuevo ambos grupos.

Por otro lado, cuando construimos nuestros datos de entrenamiento y prueba, excluimos la variable objetivo, 'diagnosis'. Para entrenar el modelo kNN, necesitaremos ahora almacenar estas etiquetas de clase en vectores de factores, divididos en los conjuntos de datos de entrenamiento y prueba:

```{r}
#fijamos la semilla para el generador pseudoaleatorio
set.seed(params$seed.train) 
# train<-sample(1:nrow(data),round(nrow(data)*params$p.train,0)) # contiene nuestra 
# selección aleatoria de datos para entrenamiento con el 67% de los datos
# creamos los grupos de entrenamiento (train) y prueba (test) con los datos normalizados
datos.train.n <- data.frame(data.norm[train,]) # 67% de los datos
datos.test.n  <- data.frame(data.norm[-train,]) # 33% de los datos

nrow(datos.train.n)/nrow(datos.test.n) # vemos que coincide con la calisifcación realizada 
# para los datos sin normalizar. empleamos por tanto la misma selección de datos,
# pero normalizados.

# Creamos las etiquetas para ambos grupos
datos.train.label <- data[train, 31]
datos.test.label <- data[-train, 31]
```

### Entrenar el modelo

Para clasificar nuestras muestras, usamos la función `knn()` para clasificar los datos del grupo test:

```{r}
library(class)
data.test.pred <- knn(train = datos.train.n, test = datos.test.n,
                      cl = datos.train.label, k = 21)
```

Como sabemos, la función `knn()` devuelve un vector factorial de etiquetas predichas para cada una de las muestras en el conjunto de datos test, que hemos asignado a `data.test.pred`.

### Predicción y Evaluación del algoritmo

El siguiente paso es evaluar qué tan bien las clases predichas en el vector `data.test.pred` coinciden con los valores conocidos en el vector` datos.test.label`. Para hacer esto, podemos usar la función `CrossTable()` en el paquete `gmodels`:

```{r}
library(gmodels)
# Podemos crear una tabulación cruzada que indique el acuerdo entre los dos vectores, 
# especificando `prop.chisq = FALSO`
conf.mat <- CrossTable(x = datos.test.label, y = data.test.pred,
           prop.chisq = FALSE)
```

Los casos `r conf.mat$t[1,1]` de `r sum(conf.mat$t)` indican casos en los que la muestra era benigna, y el algoritmo kNN la identificó correctamente. Un total de predicciones `r conf.mat$t[2,2]` de `r sum(conf.mat$t)` fueron verdaderos positivos, es decir, identificados adecuadamente como "Malignos".

Con este modelo se clasificó incorrectamente un total de `r conf.mat$t[2,1]` de las muestras `r sum (conf.mat $ t)`.

Para ver los resultados podemos emplear la función `confusionMatrix` del paquete `caret`, que nos indica los valores obtnidos de *Sensitivity* y *Specificity*. Es la función que vamos a emplear en todos los clasificadores para poder comparalos al final.

```{r}
require(caret,quietly = TRUE)
cf.knn <- confusionMatrix(data.test.pred,datos.test.label,positive="Maligno")
cf.knn
```

El modelo knn con categoria positiva 'Maligno' obtiene una precision de `r round(cf.knn$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cf.knn$byClass["Sensitivity"], 3)` y `r round(cf.knn$byClass["Specificity"], 3)` respectivamente.

Vamos a intentar ahora otra iteración del modelo para ver si podemos mejorar el rendimiento y reducir el número de valores que se han clasificado incorrectamente, especialmente aquellos casos *falsos negativos*, ya que sería grave identificar como benigno una muestra cuando en realizad es maligna. cuando en realidad era benigno. Aunque todos los errores obtenidos deben de tratar evitarse, ya que losw falsos positivos pueden ocasionar estres al paciente o gastos innecesarios al hospital.

Para mejorar el modelo vamos a realizar dos variaciones además de implementar el modelo con el paquete `caret`. Intentaremos dos variaciones simples en el clasificador anterior. Primero, un método que reescala las variables numéricas y en segundo lugar, intentaremos varios valores del parámetro *k*.

#### Posible mejora 1: Transformación - estandarización de z

Para estandarizar nuestros valores, excepto la variable diagnosis que no es numérica, emplearemos la función `scale()` de R, que por defecto reescala los valores usando the z-score standardization y almacenamos el resultado en la variable` data.z`.

```{r}
# empleamos la función scale() para estandarizar a z-score nuestro data frame
data.z <- as.data.frame(scale(data[-31]))
# comprobamos que se ha aplicado la transformación
summary(data.z$area_mean)
```

La media de una variable estandarizada z-score siempre debe ser cero (como ocurre en nuestro caso), y el rango debe ser bastante compacto. Un z-score mayor que 3 o menor que -3 indica un valor extremadamente raro. El resumen anterior nos indica que quiza esta transformación no es la más adecuada, ya que el rango es elevado, más de 3.

Sin embargo, veamos los resultados del modelo. Para ello, volvemos a entrenar el modelo con los nuevos datos de entrenamiento transformados usando la función `knn ()`. Luego compararemos las etiquetas predichas con las etiquetas reales usando `confusionMatrix()`:

```{r}
# Creamos de nuevo los grupos de entrenamiento y test con los datos normalizados 
data.train.z <- data.z[train, ]
data.test.z <- data.z[-train, ]

# re-classify test
data.test.pred.z <- knn(train = data.train.z, test = data.test.z,
                      cl = datos.train.label, k = 21)
# Creamos la tabla de resultados
cf.knn.z <- confusionMatrix(data.test.pred.z,datos.test.label,positive="Maligno")
cf.knn.z
```

En la tabla anterior los resultados muestran una ligera disminución en el grado de precisión (Accuracy).  Por otro lado, se mejora la *Specificity* (proporción de sanos correctamente identificados) pero empeora la *Sensitivity* (detectar la enfermedad en sujetos enfermos). Por lo que no se mejoran los datos del clasificador anterior.

#### Posible mejora 2: distintos valores de k

Vamos a intentar ahora mejorar el modelo con otros valores de *k*, empleando paa ello los datos normalizados y los datos de prueba, e imprimimos una tabla resumen con los resultados obtenidosÑ

```{r}
k_valor <- params$k_value
resum <- data.frame(k_valor, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
j <- 0
for (i in k_valor){
  j <- j +1
  data.test.pred.k <- knn(train = datos.train.n, test = datos.test.n, cl = datos.train.label, k=i)
  cf.knn.k <- confusionMatrix(data.test.pred.k,datos.test.label,positive="Maligno")
  
  resum[j,2:5] <- c(round(cf.knn.k$overall["Accuracy"], 3), round(cf.knn.k$overall["Kappa"], 
                                                                  3), 
                    round(cf.knn.k$byClass["Sensitivity"], 3), 
                    round(cf.knn.k$byClass["Specificity"], 3))
}
print(resum)
```

Aunque el primer clasificador empleado no ha sido perfecto, el enfoque 1NN pudo evitar algunos de los falsos negativos a expensas de agregar falsos positivos.

## Naive Bayes
### Transformación de los datos

Calcula las probabilidades a-posteriores condicionales de una variable de clase categórica dadas otras variables predictoras independientes usando la regla de Bayes, sin necesidad de ningún tipo de transformación en los datos.

### Entrenar el modelo

Como ya tenemos los grupos de train y training creados sin los datos transformados en el apartado 3.3, ya podemos entrenar el algoritmo. El valor predeterminado del argumento `laplace` (0) deshabilita el suavizado de Laplace:

```{r}
library(e1071)
data.nb <- naiveBayes(datos.train, datos.train.label, laplace=0)
```

El resultado del entrenamiento contiene las probabilidades condicionadas de cada categoria según el tipo de diagnostico. Veamos las cuatro primeras variables:

```{r}
data.nb$tables[1:4]
```

### Predicción y Evaluación del algoritmo

Aplicamos la función `predict` del algoritmo con los datos de test para hacer su predicción:

```{r}
test.pred.nb <- predict(data.nb, datos.test)
```

Ahora miramos los resultados en una *Cross Table* usando la función `confusionMatrix` del package `caret`:

```{r}
require(caret,quietly = TRUE)
nb.matrix <- confusionMatrix(test.pred.nb,datos.test.label,positive="Maligno")
nb.matrix
```

El modelo Naive Bayes con categoria positiva 'Maligno' obtiene una precisión de `r round(nb.matrix$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(nb.matrix$byClass["Sensitivity"], 3)` y `r round(nb.matrix$byClass["Specificity"], 3)` respectivamente.

#### Posible mejora: *laplace = 1*

Ahora se prueba a entrenar el modelo aplicando la opción `laplace = 1`:

```{r}
data.nb2 <- naiveBayes(datos.train, datos.train.label, laplace=1)
```

y se hace la predicción:

```{r}
test.pred.nb2 <- predict(data.nb2, datos.test)
```

Ahora se evalua el modelo con la función `confusionMatrix` del package `caret`.

```{r}
nb.matrix.1 <- confusionMatrix(test.pred.nb2,datos.test.label,positive="Maligno")
```

El nuevo modelo daplicando la opción `laplace = 1` obtiene una precision de `r round(nb.matrix.1$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(nb.matrix.1$byClass["Sensitivity"], 3)` y `r round(nb.matrix.1$byClass["Specificity"], 3)` respectivamente. `r if(nb.matrix$overall["Accuracy"] > nb.matrix.1$overall["Accuracy"]){"Vemos que el modelo obtenido con SVM lineal tiene una mayor precision"}``r if(nb.matrix$overall["Accuracy"] < nb.matrix.1$overall["Accuracy"]){"Vemos que el modelo obtenido con SVM gaussiano tiene una mayor precision"}``r if(nb.matrix$overall["Accuracy"] == nb.matrix.1$overall["Accuracy"]){"Vemos que ambos modelos tienen la misma precision"}`

#### Curvas ROC

Se presentan las curvas ROC para el modelo de Naive Bayes con `laplace=0` y `laplace= 1`.

**Caso `laplace=0`**

El primer paso es obtener las probabilidades diagnosis (Maligno/Benigno) para cada muestra de los datos de test:
```{r}
test.pred.nb3 <- predict(data.nb, datos.test, type="raw")
tail(test.pred.nb3)
```

Con la información de las probabilidades de la clase positiva ("Maligno" o 2) se construye la curva ROC.
```{r}
require(ROCR,quietly=TRUE)

pred <- prediction(predictions= test.pred.nb3[,2], labels=datos.test.label)
perf <- performance(pred, measure="tpr", x.measure="fpr")

plot(perf, main= "ROC curve Laplace 0", col= "blue", lwd=3, colorize=TRUE)
abline(a=0, b= 1, lwd= 2, lty = 2)
perf.auc <- performance(pred, measure ="auc")
```

El area bajo la curva es **`r unlist(perf.auc@y.values)`**.

**Caso `laplace=1`**

El primer paso es obtener las probabilidades diagnosis (Maligno/Benigno) para cada muestra de los datos de test:

```{r}
test.pred.nb4 <- predict(data.nb2, datos.test, type="raw")
tail(test.pred.nb4)
```

Con la información de las probabilidades de la clase positiva (1) se construye la curva ROC.

```{r}
pred2 <- prediction(predictions= test.pred.nb4[,2], labels=datos.test.label)
perf2 <- performance(pred2, measure="tpr", x.measure="fpr")

plot(perf2, main= "ROC curve Laplace 1", col= "blue", lwd=3, colorize=TRUE)
abline(a=0, b= 1, lwd= 2, lty = 2)
perf2.auc <- performance(pred2, measure ="auc")

unlist(perf2.auc@y.values)
```

El area bajo la curva es **`r unlist(perf2.auc@y.values)`** y con `laplace = 0` es **`unlist(perf.auc@y.values)`**. Como vemos no mejora la predicción, por lo que no tiene sentido cambiar el argumento de 0 a 1.

## Artificial Neural Network
### Transformación de los datos

Hay que normalizar las variables para que tomen valores entre 0 y 1, tal y como hemos realizado en la clasificación de k-nn. Los datos normalizados se encuentran dentro de `data.norm`

```{r}
#data.norm <- as.data.frame(lapply(data[,-31], normalize))
```

Se confirma que el rango de valores esta entre 0 y 1.
```{r}
summary(data.norm)[,1:5]
```

Ahora se crean tantas variables binarias como categorias tiene la variable `diagnosis`. Por lo que ahora nuestra matriz de datos tiene 32 variables, las 30 variables numéricas, y las dos variables que creamos que son de tipo binario, con valores de TRUE y/o FALSE.

```{r}
# Creación de variables binarias en lugar de usar la variable factor
data.norm$M <- data$diagnosis=="Maligno"
data.norm$B <- data$diagnosis=="Benigno"
```

Del mismo modo emplearemos los conjuntos para entrenamiento y prueba creados en el apartado anterior con datos normalizados, pero incluyendo ahora las dos nuevas variables:

```{r}
# creamos las etiquetas para los grupos de training and test
datos.train.nn <- data.frame(data.norm[train,])
datos.test.nn  <- data.frame(data.norm[-train,])

datos.train.label <- data[train, 31]
datos.test.label <- data[-train, 31]
```

### Entrenar el modelo 

Para la construcción de la red neuronal artificial se usa la función `neuralnet()` del paquete *neuralnet*. La fórmula del modelo tiene `r ncol(data.norm)` nodos de entrada y 2 (niveles de `data$diagnosis`) nodos de salida:

```{r formula, message=FALSE, warning=FALSE}
# Creamos una formula para desarrollar el modelo 
# con todas y cada una de las 32 variables:
xnam <- names(data[1:30])
(fmla <- as.formula(paste("M + B ~ ", paste(xnam, collapse= "+"))))
```

El modelo aplicado es de un nodo en la capa oculta, esto se consigue con el argumento `hidden=1`. El modelo se construye con el argumento `linear.output=FALSE` ya que se trata de un problema de clasificación.

```{r fig.height=5, message=FALSE, warning=FALSE, tidy=TRUE}
# simple ANN con una única neurona en la capa oculta
set.seed(params$seed.clsfier) # semilla que nos garantiza resultados reproducibles
library(neuralnet)
nn.model.1 <- neuralnet(fmla,
                          data = datos.train.nn,
                          hidden=1, linear.output=FALSE)
# Visualizamos la topología de la red neuronal:
plot(nn.model.1, rep='best', main="ANN con un nodo")
```

### Predicción y Evaluación del algoritmo

Una vez obtenido el primer modelo, se evalua su rendimiento con los datos de test. Se debe de clasificar las muestras test con la función `predict` al igual que en los algoritmos anteriores.
El resultado de la matriz de confusión con los datos de test en este modelo podemos obtenerla con el siguiente código:

```{r message=FALSE, warning=FALSE, tidy=TRUE}
nn.model.1.matrix <- predict(nn.model.1, datos.test.nn[,1:32])

# Creamos una salida binaria múltiple a nuestra salida categórica
max.idx <- function(xxy) {
  return(which(xxy == max(xxy)))
}
idx1 <- apply(nn.model.1.matrix, 1, max.idx)
prediction <- c('Maligno', 'Benigno')[idx1]
result <- table(prediction, data$diagnosis[-train]) # comparar con los datos test

# Resultados
library(caret)
(cmatrix1 <- confusionMatrix(result,positive = "Maligno"))
```

El modelo de una capa con categoria positiva 'Maligno' obtiene una precision de `r round(cmatrix1$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cmatrix1$byClass["Sensitivity"], 3)` y `r round(cmatrix1$byClass["Specificity"], 3)` respectivamente.

#### Posible mejora: 3 nodos

Se plantean *3 nodos* en la capa oculta para tratar de mejorar el rendimiento.

```{r warning=FALSE,message=FALSE,tidy=TRUE,fig.height=5  }

# ANN simple con tres neuronas ocultas
set.seed(params$seed.clsfier) # garantiza los resultados reproducibles
nn.model.3 <- neuralnet(fmla,
                        data = datos.train.nn,
                        hidden=3,linear.output=FALSE)
# visualizamos la topología de la red
plot(nn.model.3, rep='best', main="ANN con tres nodos")
```

El resultado de la matriz de confusión con los datos de test es:

```{r message=FALSE, warning=FALSE, tidy=TRUE}
nn.model.3.matrix <- predict(nn.model.3, datos.test.nn[,1:32])

idx3 <- apply(nn.model.3.matrix, 1, max.idx)
prediction3 <- c('Maligno', 'Benigno')[idx3]
result.3 <- table(prediction3, data$diagnosis[-train]) # comparar con los datos test

# Resultados
(cmatrix3 <- confusionMatrix(result.3,positive = "Maligno"))
```

El nuevo modelo con 3 nodos ocultos obtiene una precisión de `r round(cmatrix3$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cmatrix3$byClass["Sensitivity"], 3)` y `r round(cmatrix3$byClass["Specificity"], 3)` respectivamente. `r if(cmatrix1$overall["Accuracy"] > cmatrix3$overall["Accuracy"]){"Vemos que el modelo obtenido con un solo nodo tiene una mayor precision"}``r if(cmatrix1$overall["Accuracy"] < cmatrix3$overall["Accuracy"]){"Vemos que el modelo obtenido con tres nodo tiene una mayor precision"}``r if(cmatrix1$overall["Accuracy"] == cmatrix3$overall["Accuracy"]){"Vemos que ambos modelos tienen la misma precision"}`.

Si se parecen debemos emplear el modelo más sencillo para evitar overfitting.

### Paquete *caret*: modelo `nnet`

La función `nnet` admite datos de tipo factor, así que no hay que transformar la variable `diagnosis` en variables binarias. Empleamos los datos de entrenamiento y test creados en el apartado 3.3.
Veamos como responde el modelo con los datos de test y train:

```{r warning=FALSE,message=FALSE, fig.height=5}
# modelo Train/test sin repetición
library(caret)
library(NeuralNetTools)
model.nnet <- train(diagnosis ~ ., datos.train, method='nnet', 
               trControl= trainControl(method='none'), 
               preProcess = "range", # probamos con un preproceso de los datos
               tuneGrid= NULL, tuneLength=1 ,trace = FALSE) #

plotnet(model.nnet, main="modelo `nnet` con caret")
#summary(model.nnet)
prediction.nnet <- predict(model.nnet, datos.test)         # predecir los resultados
result.caret1 <- table(prediction.nnet, datos.test$diagnosis) # comparar con los datos test
result.caret1

# la función predict también puede devolver la probabilidad para cada clase:
prediction.nnet1 <- predict(model.nnet, datos.test, type="prob")  
head(prediction.nnet1)

# Resultados
(cmatrix.nnet <- confusionMatrix(result.caret1,positive = "Maligno"))
```

El modelo empleando `caret` con categoria positiva 'Maligno' obtiene una precision de `r round(cmatrix.nnet$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cmatrix.nnet$byClass["Sensitivity"], 3)` y `r round(cmatrix.nnet$byClass["Specificity"], 3)` respectivamente.

Empleamos ahora **5-fold crossvalidation** para tratar de mejorar la clasificación:

```{r fig.height=5, message=FALSE, warning=FALSE}
# modelo 5-crossvalidation 
model.nnet.2 <- train(diagnosis ~ ., datos.train, method='nnet', 
               trControl= trainControl(method='cv', number=5), 
               tuneGrid= NULL, tuneLength=10 ,trace = FALSE)

plotnet(model.nnet.2, alpha=0.6)
#summary(model.nnet.2)
prediction.nnet2 <- predict(model.nnet.2, datos.test)           # predecir los resultados
result.caret2 <- table(prediction.nnet2, datos.test$diagnosis)  # comparar con los datos test
result.caret2

# la probabilidad para cada clase:
prediction.nnet2 <- predict(model.nnet.2, datos.test, type="prob")  
head(prediction.nnet2)

(cmatrix.nnet2 <- confusionMatrix(result.caret2,positive = "Maligno"))
```

El modelo empleando `caret`  con 5-fold crossvalidation obtiene una precision de `r round(cmatrix.nnet2$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cmatrix.nnet2$byClass["Sensitivity"], 3)` y `r round(cmatrix.nnet2$byClass["Specificity"], 3)` respectivamente.

Como vemos con el paquete `caret` obtenemos los mejores valores si no realizamos la 5-fold crossvalidation.

## Support Vector Machine
### Transformación de los datos

El SVM no requiere realizar transformaciones normalizantes de los datos.

El porcentaje de muestras tumorales en los datos de training es de `r round((table(datos.train$diagnosis)["Maligno"]/ nrow(datos.train))*100,2)`%, y en los de test de `r round((table(datos.test$diagnosis)["Maligno"]/ nrow(datos.test))*100,2)`%. Así que, como el tamaño de la muestra es más bien bajo, se podría escoger otros metodos como k-fold cross validation o bootstrap, que emplearemos después.

### Entrenar el modelo

El algoritmo de SVM que se usa es la función `ksvm()` del paquete *kernlab*. Se construye el modelo más sencillo, el lineal, usando como kernel el valor `vanilladot`:

```{r message=FALSE, warning=FALSE}
library(kernlab)
set.seed(params$seed.clsfier)
modeloLineal <- ksvm(diagnosis~.,data=datos.train, kernel='vanilladot')

# Veamos la información básica sobre el modelo
modeloLineal
```

Se puede observar que la función lineal no tiene parametros adicionales ('hiperparametros') al de coste.

### Predicción y Evaluación del algoritmo.

Para evaluar el modelo y analizar su rendimiento se realiza la predicción con nuevos datos: los datos de test.

```{r }
modLineal.pred <- predict(modeloLineal, datos.test)
```

Se obtiene la matriz de confusión, usando la función `confusionMatrix`, al igual que en los otros algoritmos:

```{r warning=FALSE,message=FALSE}
# Modelo lineal
res.svm <- table(modLineal.pred, datos.test$diagnosis)
(cmatrixSVM <- confusionMatrix(res.svm, positive="Maligno"))
```

El modelo de SVM lineal con categoria positiva 'Maligno' obtiene una precisión de `r round(cmatrixSVM$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cmatrixSVM$byClass["Sensitivity"], 3)` y `r round(cmatrixSVM$byClass["Specificity"], 3)` respectivamente.

#### Posible mejora: kernel Gaussiano

Ahora se plantea un SVM con un el kernel Gaussiano, `rbfdot` para tratar de mejorar el rendimiento:

```{r message=FALSE, warning=FALSE, tidy=TRUE}
set.seed(params$seed.clsfier)
modeloGauss <- ksvm(diagnosis~.,data=datos.train, 
                     kernel='rbfdot')
# Predicción
modeloGauss.pred <- predict(modeloGauss, datos.test)
```

El resultado de la matriz de confusión con los datos de test es:

```{r message=FALSE, warning=FALSE, tidy=TRUE}
res.svm.gaus <- table(modeloGauss.pred, datos.test$diagnosis)
# Resultados
library(caret)
(cmatrixSVM2 <- confusionMatrix(res.svm.gaus,positive="Maligno"))
```

El nuevo modelo de SVM con kernel gaussiano obtiene una precision de `r round(cmatrixSVM2$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(cmatrixSVM2$byClass["Sensitivity"], 3)` y `r round(cmatrixSVM2$byClass["Specificity"], 3)` respectivamente. `r if(cmatrix1$overall["Accuracy"] > cmatrixSVM2$overall["Accuracy"]){"Vemos que el modelo obtenido con SVM lineal tiene una mayor precision"}``r if(cmatrix1$overall["Accuracy"] < cmatrixSVM2$overall["Accuracy"]){"Vemos que el modelo obtenido con SVM gaussiano tiene una mayor precision"}``r if(cmatrix1$overall["Accuracy"] == cmatrixSVM2$overall["Accuracy"]){"Vemos que ambos modelos tienen la misma precision"}`.

### Paquete *caret*

Se construye un nuevo modelo con el objetivo de comprobar si la implementación del paquete `caret` produce diferencias. Se evalúa el rendimiento con la partición train/test y 5-fold cross validation en el modelo lineal y bootstrap con el modelo radial.

Para crear la partición de los datos en training/test tenemos la posibilidad de emplear la función `createDataPartition` del paquete `caret`. Como curiosidad veamos que ocurre con los resultados, pero posteriormente emplearemos el mismo set de datos de training/test creados en el apartado 3.3, para poder comprar los resultados.

```{r warning=FALSE,message=FALSE}
library(caret)
# Partición de datos
set.seed(params$seed.train)
# We wish 75% for the trainset 
inTrain <- createDataPartition(y=data$diagnosis, p=params$p.train, list=FALSE)

train.set <- data[inTrain,]
test.set  <- data[-inTrain,]

nrow(train.set)/nrow(test.set) # debe estar alrededor de 2
```

Entrenamos el modelo con los datos de train y test creados por el paquete `caret`:

```{r warning=FALSE,message=FALSE}
# modelo sólo de Train sin repetición
set.seed(params$seed.otro)
model.svm.caret <- train(diagnosis ~ ., train.set, method='svmLinear', 
               trControl= trainControl(method='none'), 
                tuneGrid= NULL, trace = FALSE) #

prediction.svm.caret <- predict(model.svm.caret, test.set)         # predecir los resultados
res.svm.caret <- table(prediction.svm.caret, test.set$diagnosis)   # comparar con los datos test

confusionMatrix(res.svm.caret, positive="Maligno")   
```

Ahora se repite el modelo pero con los mismos datos de training/test usados en la función `ksvm` con kernel `vanilladot` del paquete *kernlab*, para comprobar las variaciones:

```{r warning=FALSE,message=FALSE}
# modelo sin repetición
set.seed(params$seed.otro)
model.svm.caret2 <- train(diagnosis ~ ., datos.train, method='svmLinear', 
               trControl= trainControl(method='none'), 
                tuneGrid= NULL, trace = FALSE)

prediction.svm.caret2 <- predict(model.svm.caret2, datos.test)          # predecir los resultados
res.svm.caret2 <- table(prediction.svm.caret2, datos.test$diagnosis)    # comparar con los datos test

svm.caret.matrix <- confusionMatrix(res.svm.caret2, positive="Maligno")   
```

El modelo de SVM lineal con categoria positiva 'Maligno' obtiene una precisión de `r round(svm.caret.matrix$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(svm.caret.matrix$byClass["Sensitivity"], 3)` y `r round(svm.caret.matrix$byClass["Specificity"], 3)` respectivamente.

Como vemos los datos empleados en el análisis son importantes ya que pueden cambiar el resultado de los datos. En este caso la selección de datos empleada por el paquete `caret` mejora tanto la precisión como la sensibilidad y la especificidad.

Empleamos ahora **5-fold crossvalidation** para tratar de mejorar la clasificación:

```{r warning=FALSE,message=FALSE}
# modelo 5-crossvalidation 
set.seed(params$seed.clsfier)
model.svm.fold <- train(diagnosis ~ ., datos.train, method='svmLinear', 
               trControl= trainControl(method='cv', number=5), 
                tuneGrid= NULL, trace = FALSE)

prediction.fold <- predict(model.svm.fold, datos.test)      # predecir los resultados
res.fold <- table(prediction.fold, datos.test$diagnosis)    # comparar con los datos test

svm.caret.matrix5 <- confusionMatrix(res.fold, positive="Maligno")   
```

El modelo de SVM lineal con categoria positiva 'Maligno' obtiene una precisión de `r round(svm.caret.matrix5$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(svm.caret.matrix5$byClass["Sensitivity"], 3)` y `r round(svm.caret.matrix5$byClass["Specificity"], 3)` respectivamente.

Obtenemos datos similares empleando el paquete `caret` pero sin 5-fold crossvalidation. 

Por último empleamos bootstrap con el modelo SVM `svmRadial`:

```{r warning=FALSE,message=FALSE}
# Por defecto es Bootstrap, con 25 repeticiones para 3 posibles decay
# y 3 posibles sizes
set.seed(params$seed.clsfier)
model.svm.caret3 <- train(diagnosis ~ ., datos.train, method='svmRadial', trace = FALSE)

prediction.boots <- predict(model.svm.caret3, datos.test)      # predecir los resultados
res.svm.boots <- table(prediction.boots, datos.test$diagnosis) # comparar con los datos test

res.svm.boots.matrix <- confusionMatrix(res.svm.boots, positive="Maligno")
res.svm.boots.matrix
```

El modelo de SVM radial con validación bootstrap obtiene una precisión de `r round(res.svm.boots.matrix$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(res.svm.boots.matrix$byClass["Sensitivity"], 3)` y `r round(res.svm.boots.matrix$byClass["Specificity"], 3)` respectivamente.

## Árbol de Decisión
### Transformación de los datos

El algorítmo de árboles de decisión no requiere realizar transformaciones normalizantes de los datos, empleamos los datos de entrenamiento y test creados en el partado 3.3.

### Entrenar el modelo 

```{r}
# fijamos la semilla para el clasificador
library(C50)
set.seed(params$seed.clsfier)
data.model.ab<- C5.0(datos.train[-31], datos.train$diagnosis)
data.model.ab
```

Para ver todos los detalles del árbol de decisiones creado podemos usar **summary**:

```{r}
res.ab<-summary(data.model.ab)
res.ab
```

El resultado anterior muestra las ramas en el árbol de decisión. Después del árbol, el resultado del resumen muestra una **matriz de confusión**, que nos indica los registros incorrectamente clasificados por el modelo. Se observa que este modelo clasifica adecuadamente el diagnóstico de todas las muestras excepto 1 de las `r data.model.ab$dims[1]` muestras, con una tasa de error del 1,1%. Un total de 134 valores reales no se clasificaron incorrectamente como  (falsos positivos), mientras que 3 valores se clasificaron erróneamente como Malignos (falsos negativos).

```{r}
plot(data.model.ab, subtree=2) 
```

### Predicción y Evaluación del algoritmo

La salida anterior debemos validarla con los datos de test:

```{r}
data.predict.ab<- predict(data.model.ab, datos.test)
```

Creamos la tabla de resultados con la función `confusionMatrix()` de `caret`:

```{r}
cf.arbol<-confusionMatrix(data.predict.ab, datos.test$diagnosis, positive = "Maligno")
cf.arbol
```

De las `r length(datos.test$default)` muestras incluidas en los datos de test, nuestro modelo predijo correctamente que `r cf.arbol$t[1,1]` como **Maligno** y `r cf.arbol$t[2,2]` como **Benigno**, lo que resulta una precisión del `r round((cf.arbol$prop.tbl[1,1]+cf.arbol$prop.tbl[2,2])*100,2)`%  y una tasa de error del `r round((cf.arbol$prop.tbl[1,2]+cf.arbol$prop.tbl[2,1])*100,2)`%. 

Este resultado es peor que su rendimiento en los datos de entrenamiento, pero no es inesperado, dado que el rendimiento de un modelo a menudo es peor en datos no vistos.

Si nos fijamos en **Sensitivity**, sensibilidad del modelo, vemos que es `r round(cf.arbol$byClass[1],2)`, y si nos fijamos en **Specificity**, ratio de verdaderos negativos, que nos mide la proporción de negativos que han sido correctamente clasificados,  vemos que es `r round(cf.arbol$byClass[2],2)`.

En resumen:

```{r echo=FALSE, message=FALSE, warning=FALSE}
resum <- data.frame(h=NA, accu=NA, kap=NA, Sens=NA)
resum[1,1:4] <- c("C5.0",round(cf.arbol$overall[1],3),round(cf.arbol$overall[2],3), round(cf.arbol$byClass[1],2))
kable(resum[,1:4], col.names=c("modelo", "Accuracy", "Kappa", "Sensitivity"),
      align= c("l","c","c","c"), caption = "ANÁLISIS USANDO ÁRBOLES DE CLASIFICACIÓN")
```

#### Posible mejora: algoritmo C 4.5

Comprobamos si empleando el algoritmo C 4.5,  mediante la adición de refuerzo adaptativo, mejora el modelo. Este es un proceso en el que se construyen muchos árboles de decisión y los árboles votan en la mejor clase para cada ejemplo.

La función `C5.0()` facilita agregar aumentos a nuestro árbol de decisión C5.0. Simplemente necesitamos agregar un parámetro adicional **trials** que indique el **número de árboles de decisión separados** para usar en el modelo. Este parámetro establece un límite superior; el algoritmo dejará de agregar árboles si reconoce que las pruebas adicionales no parecen mejorar la precisión. Comenzaremos con `trials=10` , un número qestándar, ya que las investigaciones sugieren que esto reduce las tasas de error en los datos de prueba en aproximadamente un 25 por ciento:

```{r}
# fijamos la semilla para el clasificador
set.seed(params$seed.clsfier)
model.boost10 <- C5.0(datos.train[-31], datos.train$diagnosis, trials = 10)
summary(model.boost10)
```

El clasificador cometió un error de 4 en `r nrow(datos.train)` registros de datos de entrenamiento para una tasa de error de `r round((4/nrow(datos.train))*100,2)` por ciento. Representa una gran mejora con respecto a la tasa de error del modelo anterior antes de agregar potenciación. Veamos queda por ver si hay una mejora en los datos de prueba:

```{r}
model.boost.pred10 <- predict(model.boost10, datos.test)

cf.arbol.10<-confusionMatrix(model.boost.pred10, datos.test$diagnosis, positive = "Maligno")
cf.arbol.10
```

La precisión se ha mantenido respecto al modelo C5.0 sin boosting. Sin embargo la sensibilidad mejora levemente.

Accuracy es de `r round(cf.arbol.10$overall["Accuracy"],3)`  
Kappa es de `r round(cf.arbol.10$overall["Kappa"],3)`

### Paquete *caret*

Empleamos los mismos grupos de train y test que hemos empleado en el análisis anterior.
Para ajustar el modelo se emplea la función `train` y empleamos 5-fold crossvalidation para entrenarlo:

```{r message=FALSE, warning=FALSE}
## método : cv K-fold cross validation
## número : K folds
ctrl <- trainControl( method="cv",
                      number=5,
                      selectionFunction= "oneSE") 

## Parámetros de Grid para algortimo de C50
## trials -> número de iteraciones boosting 
grid_C50 <- expand.grid(model="tree", trials=c(5,10,20,30),winnow="FALSE")

# fijamos la semilla para el clasificador
set.seed(params$seed.clsfier)
## trace <- FALSE para suprimir las iteraciones de entrenamiento
modeloC5.0     <- train (diagnosis ~ .,
                  data = datos.train,
                  method ="C5.0",
                  trControl=ctrl,
                  tuneGrid = grid_C50, 
                  metric="Accuracy",
                  prePoc = c("center", "scale"),
                  verbose =FALSE,
                  trace = FALSE)
modeloC5.0
```

para la evaliuación del rendimiento creamos la matriz de resultados:

```{r}
# Predicción de Clases
prd.c50 <- predict ( modeloC5.0, newdata = datos.test)
# Matriz de Confusión
(cf.c50 <- confusionMatrix( data=prd.c50, datos.test$diagnosis, positive="Maligno"))
```

La precisión ha aumentado respecto al modelo anterior sin `caret`. 

Accuracy es de `r round(cf.c50$overall["Accuracy"],3)`  
Kappa es de `r round(cf.c50$overall["Kappa"],3)`.

## Random Forest
### Transformación de los datos

El algorítmo de  *Random Forest* no requiere realizar transformaciones normalizantes de los datos, empleamos los datos de entrenamiento y test creados en el partado 3.3 y empleados en la mayoria de los algoritmos.

### Entrenar el modelo 

La función `randomForest()` crea por defecto un conjunto de 500 árboles, donde cada uno de ellos elige $\sqrt(p)$ variables de forma aleatoria para cada árbol, donde `p` es el número de variables en el conjunto de datos de entrenamiento. 

El objetivo de utilizar una gran cantidad de árboles es entrenar lo suficiente para que cada variable tenga la oportunidad de aparecer en varios modelos, base del parámetro `mtry`. El uso de este valor limita las variables lo suficiente como para que ocurra una variación aleatoria sustancial de árbol a árbol. 

Empleamos como en los casos antiores la variable factor "diagnosis" para que el método ejecutado sea **classification**, de lo contrario se ejecuta **regresion**:

```{r}
library(randomForest)
set.seed(params$seed.clsfier)
randomf <- randomForest(diagnosis ~ ., data  = datos.train)
randomf
```

El resultado indica que *Random Forest* incluyó `r randomf$ntree` árboles y probó `r randomf$mtry` variables en cada división. Hagamos un gráfico del modelo:

```{r}
plot(randomf, main="modelo Random Forest")
```

Donde la línea roja representa el error cometido al intentar predecir el diagnóstico, y la línea verde es el error en la predicción **Benigno**. Podemos ver la mmportancia de cada una de las variables en la clasificación del modelo:

```{r}
head(importance(randomf))
varImpPlot(randomf, main="Variables Random Forest")
```

### Predicción y Evaluación del algoritmo

Creamos para ello la matriz de los resultados obtenidos en la predicción del diagnóstico:

```{r}
library(caret)
predict.rf<- predict(randomf, datos.test)
rf.matrix <- confusionMatrix(datos.test$diagnosis, predict.rf, positive = "Maligno")
rf.matrix
```

Como vemos la precisión (Accuracy) es de `r round(rf.matrix$overall["Accuracy"],3)`  y Kappa es de `r round(rf.matrix$overall["Kappa"],3)`.

#### Posible mejora: aumento del número de árboles

Veamos si incrementando el número de árboles el modelo mejora. Entrenamos de nuevo el modelo:

```{r}
set.seed(params$seed.clsfier)
rf.1000 <- randomForest(diagnosis ~ ., data  = datos.train, ntree=1000)
rf.1000
plot(rf.1000, main="modelo Random Forest 1000 árboles")
```

```{r}
predict.rf<- predict(rf.1000, datos.test)
rf.matrix1000 <- confusionMatrix(datos.test$diagnosis, predict.rf,positive="Maligno")
rf.matrix1000
```

El nuevo modelo obtiene una precisión de `r round(rf.matrix1000$overall["Accuracy"], 3)` y una sensitividad y especificidad de `r round(rf.matrix1000$byClass["Sensitivity"], 3)` y `r round(rf.matrix1000$byClass["Specificity"], 3)` respectivamente. `r if(rf.matrix$overall["Accuracy"] > rf.matrix1000$overall["Accuracy"]){"Vemos que el modelo obtenido con un solo nodo tiene una mayor precision"}``r if(rf.matrix$overall["Accuracy"] < rf.matrix1000$overall["Accuracy"]){"Vemos que el modelo obtenido con tres nodo tiene una mayor precision"}``r if(rf.matrix$overall["Accuracy"] == cmatrix3$overall["Accuracy"]){"Vemos que ambos modelos tienen la misma precision"}`.

### Paquete *caret*

Se vuelve a analizar el mismo dataset pero ahora usando el modelo `rf` del paquete `caret`. Y lo validadmos con 5-fold cross validation. Como sabemos, el fichero de train contiene `r nrow(datos.train)` observaciones y el de test `r nrow(datos.test)`.

```{r }
## método : repeatedcv K-fold cross validation
## number : K folds
## repeats : número de repeticiones
set.seed(params$semilla)
ctrl2 <- trainControl( method="repeatedcv",
                      number=5,
                      summaryFunction = defaultSummary,
                      verboseIter =FALSE,
                      repeats=3)

## Tunegrid para Random Forest
# mtry define cuántas variables se seleccionan al azar en cada split. Por 
#  defecto sqrt(n.variables)
grid_rf <- expand.grid(mtry = c(2,4,8,16))

## trace <- FALSE para suprimir las iteraciones de entrenamiento
modelo.rf.caret     <- train (diagnosis ~ .,
                  data = datos.train,
                  method ="rf",
                  trControl=ctrl2,
                  tuneGrid = grid_rf,
                  metric="Accuracy",
                  prePoc = c("center", "scale"),
                  verbose =FALSE,
                  trace = FALSE)
```

para la evaliuación del rendimiento creamos la matriz de resultados:

```{r }
# Predicción del diagnóstico
predict.rf2 <- predict(modelo.rf.caret, newdata = datos.test)
# Matriz de Confusión
(cfrf <- confusionMatrix(data=predict.rf2, datos.test$diagnosis, positive="Maligno"))

```

La precisión es similar al modelo anterior sin el paquete `caret`. 

Accuracy es de `r round(cfrf$overall["Accuracy"],3)`  
Kappa es de `r round(cfrf$overall["Kappa"],3)`  

# Resumen de resultados

En la siguiente tabla se muestra un resumen con los principales parámetros obtenidos de la función `confusionMatrix` del paquete `caret` en cada uno de los algoritmos:

```{r message=FALSE, warning=FALSE, include=FALSE}
resum1 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum1[1,1:5] <- c("knn",round(cf.knn$overall[1],3),round(cf.knn$overall[2],3), round(cf.knn$byClass[1],2), round(cf.knn$byClass[2],3))
tabla1 <- as.data.frame(resum1[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum2 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum2[1,1:5] <- c("knn z transf",round(cf.knn.z$overall[1],3),round(cf.knn.z$overall[2],3), round(cf.knn.z$byClass[1],2), round(cf.knn.z$byClass[2],3))
tabla2 <- as.data.frame(resum2[,1:5])
```

```{r include=FALSE}
modelo <- params$k_value
resum <- data.frame(modelo, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
j <- 0
for (i in k_valor){
  j <- j +1
  data.test.pred.k <- knn(train = datos.train.n, test = datos.test.n, cl = datos.train.label, k=i)
  cf.knn.k <- confusionMatrix(data.test.pred.k,datos.test.label,positive="Maligno")
  
  resum[j,2:5] <- c(round(cf.knn.k$overall["Accuracy"], 3), round(cf.knn.k$overall["Kappa"], 3), round(cf.knn.k$byClass["Sensitivity"], 3), round(cf.knn.k$byClass["Specificity"], 3))
}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum3 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum3[1,1:5] <- c("Naive-Bayes L0",round(nb.matrix$overall[1],3),round(nb.matrix$overall[2],3), round(nb.matrix$byClass[1],2), round(nb.matrix$byClass[2],3))
tabla3 <- as.data.frame(resum3[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum4 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum4[1,1:5] <- c("Naive-Bayes L1",round(nb.matrix.1$overall[1],3),round(nb.matrix.1$overall[2],3), round(nb.matrix.1$byClass[1],2), round(nb.matrix.1$byClass[2],3))
tabla4 <- as.data.frame(resum4[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum5 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum5[1,1:5] <- c("ANN 1 nodo",round(cmatrix1$overall[1],3),round(cmatrix1$overall[2],3), round(cmatrix1$byClass[1],2), round(cmatrix1$byClass[2],3))
tabla5 <- as.data.frame(resum5[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum6 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum6[1,1:5] <- c("ANN 3 nodos",round(cmatrix3$overall[1],3),round(cmatrix3$overall[2],3), round(cmatrix3$byClass[1],2), round(cmatrix3$byClass[2],3))
tabla6 <- as.data.frame(resum6[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum7 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum7[1,1:5] <- c("ANN caret",round(cmatrix.nnet$overall[1],3),round(cmatrix.nnet$overall[2],3), round(cmatrix.nnet$byClass[1],2), round(cmatrix.nnet$byClass[2],3))
tabla7 <- as.data.frame(resum7[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum8 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum8[1,1:5] <- c("ANN caret 5-fold", round(cmatrix.nnet2$overall[1],3),round(cmatrix.nnet2$overall[2],3), round(cmatrix.nnet2$byClass[1],2), round(cmatrix.nnet2$byClass[2],3))
tabla8 <- as.data.frame(resum8[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum9 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum9[1,1:5] <- c("SVM Lineal", round(cmatrixSVM$overall[1],3),round(cmatrixSVM$overall[2],3), round(cmatrixSVM$byClass[1],2), round(cmatrixSVM$byClass[2],3))
tabla9 <- as.data.frame(resum9[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum10 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum10[1,1:5] <- c("SVM Gauss", round(cmatrixSVM2$overall[1],3),round(cmatrixSVM2$overall[2],3), round(cmatrixSVM2$byClass[1],2), round(cmatrixSVM2$byClass[2],3))
tabla10 <- as.data.frame(resum10[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum11 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum11[1,1:5] <- c("SVM Caret", round(svm.caret.matrix$overall[1],3),round(svm.caret.matrix$overall[2],3), round(svm.caret.matrix$byClass[1],2), round(svm.caret.matrix$byClass[2],3))
tabla11 <- as.data.frame(resum11[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum12 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum12[1,1:5] <- c("SVM Caret 5fold", round(svm.caret.matrix5$overall[1],3),round(svm.caret.matrix5$overall[2],3), round(svm.caret.matrix5$byClass[1],2), round(svm.caret.matrix5$byClass[2],3))
tabla12 <- as.data.frame(resum12[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum13 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum13[1,1:5] <- c("SVM Caret boots", round(res.svm.boots.matrix$overall[1],3),round(res.svm.boots.matrix$overall[2],3), round(res.svm.boots.matrix$byClass[1],2), round(res.svm.boots.matrix$byClass[2],3))
tabla13 <- as.data.frame(resum13[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum14 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum14[1,1:5] <- c("Arbol decision", round(cf.arbol$overall[1],3),round(cf.arbol$overall[2],3), round(cf.arbol$byClass[1],2), round(cf.arbol$byClass[2],3))
tabla14 <- as.data.frame(resum14[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum15 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum15[1,1:5] <- c("Arbol decision 10", round(cf.arbol.10$overall[1],3),round(cf.arbol.10$overall[2],3), round(cf.arbol.10$byClass[1],2), round(cf.arbol.10$byClass[2],3))
tabla15 <- as.data.frame(resum15[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum16 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum16[1,1:5] <- c("Arbol decision caret", round(cf.c50$overall[1],3),round(cf.c50$overall[2],3), round(cf.c50$byClass[1],2), round(cf.c50$byClass[2],3))
tabla16 <- as.data.frame(resum16[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum17 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum17[1,1:5] <- c("Random forest", round(rf.matrix$overall[1],3),round(rf.matrix$overall[2],3), round(rf.matrix$byClass[1],2), round(rf.matrix$byClass[2],3))
tabla17 <- as.data.frame(resum17[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum18 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum18[1,1:5] <- c("Random forest 1000", round(rf.matrix1000$overall[1],3),round(rf.matrix1000$overall[2],3), round(rf.matrix1000$byClass[1],2), round(rf.matrix1000$byClass[2],3))
tabla18 <- as.data.frame(resum18[,1:5])
```

```{r message=FALSE, warning=FALSE, include=FALSE}
resum19 <- data.frame(modelo=NA, Accuracy=NA, Kappa=NA, Sensitivity=NA, Specificity=NA)
resum19[1,1:5] <- c("Random forest caret", round(cfrf$overall[1],3),round(cfrf$overall[2],3), round(cfrf$byClass[1],2), round(cfrf$byClass[2],3))
tabla19 <- as.data.frame(resum19[,1:5])
```

```{r include=FALSE}
resultado.final <- data.frame(rbind(tabla1, tabla2, resum, tabla3, tabla4, tabla5, tabla6, tabla7, tabla8, tabla9, tabla10, tabla11, tabla12, tabla13, tabla14, tabla15, tabla16, tabla17, tabla18, tabla19))

```

Ordenamos los datos según los valores de precisión y especificidad.

```{r message=FALSE, warning=FALSE}
# Ordenados por precisión y sensibilidad
dplyr::arrange(resultado.final, Accuracy, Sensitivity)

better.results <- dplyr::filter(resultado.final, Accuracy > 0.946)
results <- dplyr::arrange(better.results, Accuracy, Sensitivity)
results
```

En la Tabla anterior se pueden observar que los clasificadores Naive-Bayes fueron los más acertados a la hora de clasificar las muestras de los pacientes, obteniendo un 96% de precisión. Por el contrario, se puede observar que los clasificadores ANN con el paquete `caret` y Árboles de decisión clasifican las muestras con una precisón del 89 % y 93% en promedio respectivamente.

# Discusión y conslusiones

El mejor modelo a aplicar dependerá de la importancia que le demos tanto a la precisión como a la sensibilidad y especificidad en la detección del cáncer de mama según nuestros resultados.

Como podemos observar en el siguiente gráfico de barras todos los modelos generados tienen una accuracy por encima del 85% siendo el modelo generado por el algoritmo Naive-Bayes con Laplace 0 el que mayor potencia de clasificación tiene con una accuracy del 96.8%.

```{r fig.width=8}
ggplot(data=resultado.final, aes(x=modelo, y=Accuracy, fill=modelo)) + 
    geom_bar(stat="identity", position="dodge") +  theme(text = element_text(size=8),
                                                         legend.position='bottom') 
```

La sensibilidad caracteriza la capacidad de la prueba para detectar la enfermedad en sujetos enfermos, lo que tras la precisión quizá debería de ser el factor a tener en cuenta y ante igualdad, un mejor valor de especificidad. El modelo con mejor precisión es *Naive-Bayes L0* sin embargo hay otros modelos con mejor sensibilidad y valores parecidos de precisión. *Random forest* tiene una elevada precisión y sensibilidad pero un valor muy bajo de `kappa`. *Arbol decision caret* tiene elevada Sensibilidad y pero menor percisión y specificidad.

Para este conjunto de datos, los modelos que han tenido menos potencia de clasificación han sido los generados por el algoritmo de Artificial Neural Network (ANN)  del paquete `caret`, por debajo del 90% de accuracy.

El algoritmo de árboles de decisión y Random Forest han tenido valores intermedios de Accuracy, sin embargo con valores elevados de Sensibility.

He encontrado diferencias significativas a la hora de generar el modelo con paquetes específicos de cada algoritmo o utilizando el paquete `caret`, especialmente con el algoritmo SVM que mejora significamente si empleamos algún método como 5-fold validation o boostrapping. En cambio con el algoritmo ANN los resultados con `caret` muestran menos potencia.

Como conslusión, según estos resultados y los objetivos que buscamos con nuestros análisis, el modelo *Naive-Bayes L0* sería el más apropiado, con un porcentaje cercano al 95% tanto en precisión como en sensibilidad y con valores de kappa y especificidad aceptables, lo que podemos apreciar observando el siguiente gráfico donde podemos compararlo con los otros modelos que han mostrado mejores resultados:

```{r fig.width=8}
ggplot(data=results, aes(x=modelo, y=Accuracy, fill=Sensitivity)) + 
    geom_bar(stat="identity", position="dodge") +  theme(text = element_text(size=8),
                                                         legend.position='bottom') 
```

# Bibliografía

