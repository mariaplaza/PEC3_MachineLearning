abline(h=5,lwd=2)
boxplot(data[,-c(3,4,14,23,24)],main='Datos sin normalizar',col='brown',cex.axis=0.7,subset=data$diagnosis)
abline(h=5,lwd=2)
boxplot(data,main='Datos sin normalizar',col='brown',cex.axis=0.7,subset=data$diagnosis)
boxplot(data[,-c(3,4,14,23,24)],main='Datos sin normalizar',col='brown',cex.axis=0.7,subset=data$diagnosis)
pairs(data[,1:6])
pairs(data[,7:12])
pairs(data[,13:18])
pairs(data[,19:24])
pairs(data[,25:30])
summary(data.norm)
boxplot(data.norm[,1:9],main='Datos con escala [0,1]',col='brown',cex.axis=0.4)
abline(h=0.5,lwd=2)
# simple ANN con una única neurona en la capa oculta
set.seed(params$seed.clsfier) # semilla que nos garantiza resultados reproducibles
library(neuralnet)
nn.model.1 <- neuralnet(fmla,
data = datos.train.n,
hidden=1, linear.output=FALSE)
# Creamos una formula para desarrollar el modelo
# con todas y cada una de las 180 variables:
xnam <- names(data[1:31])
(fmla <- as.formula(paste("diagnosis ~ ", paste(xnam, collapse= "+"))))
# simple ANN con una única neurona en la capa oculta
set.seed(params$seed.clsfier) # semilla que nos garantiza resultados reproducibles
library(neuralnet)
nn.model.1 <- neuralnet(fmla,
data = datos.train.n,
hidden=1, linear.output=FALSE)
# Ahora ya se importan los datos a formato data.frame
library(readr)
m.file <- ifelse(params$folder.data=="",
params$file,
file.path(params$folder.data,params$file))
data <- read.csv(file=m.file)
dim(data)
class(data)
head(data)
# drop the id feature
data <- data[-1]
# table of diagnosis
table(data$diagnosis)
class(data$diagnosis)
data$diagnosis <- factor(data$diagnosis, levels = c("B", "M"),
labels = c("Benignos", "Malignos"))
class(data$diagnosis)
# table or proportions with more informative labels
round(prop.table(table(data$diagnosis)) * 100, digits = 1)
# summarize three numeric features
kable(summary(data[c("radius_mean", "area_mean", "smoothness_mean")]))
#fijar la semilla para el generador pseudoaleatorio
set.seed(params$seed.train)
# create training and test data
train<-sample(1:nrow(data),round(nrow(data)*params$p.train,0))
datos.train <- data.frame(data[train,])
datos.test  <- data.frame(data[-train,])
prop.table(table(datos.train$diagnosis))
prop.table(table(datos.test$diagnosis))
# Funcion de normalizacion
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# normalizamos los datos
data.norm <- as.data.frame(lapply(data[,-31], normalize))
# confirmamos que funciona
summary(data$area_mean)
# create labels for training and test data
datos.train.n <- data.frame(data.norm[train,])
datos.test.n  <- data.frame(data.norm[-train,])
datos.train.label <- data[train, 31]
datos.test.label <- data[-train, 31]
library(class)
data.test.pred <- knn(train = datos.train.n, test = datos.test.n,
cl = datos.train.label, k = 21)
library(gmodels)
# Podemos crear una tabulación cruzada que indique el acuerdo entre los dos vectores. Especificando `prop.chisq = FALSO`
conf.mat <- CrossTable(x = datos.test.label, y = data.test.pred,
prop.chisq = FALSE)
# use the scale() function to z-score standardize a data frame
data.z <- as.data.frame(scale(data[-31]))
# comprobamos que se ha aplicado la transformacion
summary(data.z$area_mean)
# create training and test datasets
data.train.z <- data.z[train, ]
data.test.z <- data.z[-train, ]
# re-classify test cases
data.test.pred.z <- knn(train = data.train.z, test = data.test.z,
cl = datos.train.label, k = 21)
# Create the cross tabulation of predicted vs. actual
conf.mat1 <-CrossTable(x = datos.test.label, y = data.test.pred.z,
prop.chisq = FALSE)
# try several different values of k
ks <- params$k_value
resum <- data.frame(ks, FN=NA, FP=NA, mal_clas=NA)
j <- 0
for (i in ks){
j <- j +1
data.test.pred.k <- knn(train = datos.train.n, test = datos.test.n, cl = datos.train.label, k=i)
conf.mat <- CrossTable(x = datos.test.label, y = data.test.pred.k, prop.chisq=FALSE)
resum[j,2:4] <- c(conf.mat$t[2,1], conf.mat$t[1,2], ((conf.mat$t[1,2]+conf.mat$t[2,1])/sum(conf.mat$t))*100)
}
library(knitr)
kable(resum, col.names=c("k value", "# false negatives",
"# false positives", "% classified Incorrectly"),
align= c("l","c","c","c"))
library(e1071)
data.clsf <- naiveBayes(datos.train, datos.train.label, laplace=0)
data.clsf$tables[1:4]
test.pred <- predict(data.clsf, datos.test)
require(caret,quietly = TRUE)
confusionMatrix(test.pred,datos.test.label,positive="Malignos")
data.clsf2 <- naiveBayes(datos.train, datos.train.label, laplace=1)
test.pred2 <- predict(data.clsf2, datos.test)
require(caret,quietly = TRUE)
confusionMatrix(test.pred2,datos.test.label,positive="Malignos")
test.pred3 <- predict(data.clsf, datos.test, type="raw")
tail(test.pred3)
class(data$diagnosis)
data$diagnosis <- factor(data$diagnosis, levels = c("B", "M"),
labels = c("Benigno", "Maligno"))
class(data$diagnosis)
require(caret,quietly = TRUE)
confusionMatrix(test.pred,datos.test.label,positive="Maligno")
# Ahora ya se importan los datos a formato data.frame
library(readr)
m.file <- ifelse(params$folder.data=="",
params$file,
file.path(params$folder.data,params$file))
data <- read.csv(file=m.file)
dim(data)
class(data)
head(data)
# drop the id feature
data <- data[-1]
# table of diagnosis
table(data$diagnosis)
class(data$diagnosis)
data$diagnosis <- factor(data$diagnosis, levels = c("B", "M"),
labels = c("Benigno", "Maligno"))
class(data$diagnosis)
# table or proportions with more informative labels
round(prop.table(table(data$diagnosis)) * 100, digits = 1)
#fijar la semilla para el generador pseudoaleatorio
set.seed(params$seed.train)
# create training and test data
train<-sample(1:nrow(data),round(nrow(data)*params$p.train,0))
datos.train <- data.frame(data[train,])
datos.test  <- data.frame(data[-train,])
prop.table(table(datos.train$diagnosis))
# normalizamos los datos
data.norm <- as.data.frame(lapply(data[,-31], normalize))
# confirmamos que funciona
summary(data$area_mean)
# create labels for training and test data
datos.train.n <- data.frame(data.norm[train,])
datos.test.n  <- data.frame(data.norm[-train,])
datos.train.label <- data[train, 31]
datos.test.label <- data[-train, 31]
library(class)
data.test.pred <- knn(train = datos.train.n, test = datos.test.n,
cl = datos.train.label, k = 21)
library(gmodels)
# Podemos crear una tabulación cruzada que indique el acuerdo entre los dos vectores. Especificando `prop.chisq = FALSO`
conf.mat <- CrossTable(x = datos.test.label, y = data.test.pred,
prop.chisq = FALSE)
# use the scale() function to z-score standardize a data frame
data.z <- as.data.frame(scale(data[-31]))
# comprobamos que se ha aplicado la transformacion
summary(data.z$area_mean)
# create training and test datasets
data.train.z <- data.z[train, ]
data.test.z <- data.z[-train, ]
# re-classify test cases
data.test.pred.z <- knn(train = data.train.z, test = data.test.z,
cl = datos.train.label, k = 21)
# Create the cross tabulation of predicted vs. actual
conf.mat1 <-CrossTable(x = datos.test.label, y = data.test.pred.z,
prop.chisq = FALSE)
# try several different values of k
ks <- params$k_value
resum <- data.frame(ks, FN=NA, FP=NA, mal_clas=NA)
j <- 0
for (i in ks){
j <- j +1
data.test.pred.k <- knn(train = datos.train.n, test = datos.test.n, cl = datos.train.label, k=i)
conf.mat <- CrossTable(x = datos.test.label, y = data.test.pred.k, prop.chisq=FALSE)
resum[j,2:4] <- c(conf.mat$t[2,1], conf.mat$t[1,2], ((conf.mat$t[1,2]+conf.mat$t[2,1])/sum(conf.mat$t))*100)
}
library(knitr)
kable(resum, col.names=c("k value", "# false negatives",
"# false positives", "% classified Incorrectly"),
align= c("l","c","c","c"))
library(e1071)
data.clsf <- naiveBayes(datos.train, datos.train.label, laplace=0)
data.clsf$tables[1:4]
test.pred <- predict(data.clsf, datos.test)
require(caret,quietly = TRUE)
confusionMatrix(test.pred,datos.test.label,positive="Maligno")
data.clsf2 <- naiveBayes(datos.train, datos.train.label, laplace=1)
test.pred2 <- predict(data.clsf2, datos.test)
require(caret,quietly = TRUE)
confusionMatrix(test.pred2,datos.test.label,positive="Malignos")
require(caret,quietly = TRUE)
confusionMatrix(test.pred2,datos.test.label,positive="Maligno")
summary(data.norm)
boxplot(data.norm[,1:9],main='Datos con escala [0,1]',col='brown',cex.axis=0.4)
abline(h=0.5,lwd=2)
# Creaci?n de variables binarias en lugar de usar la variable factor
data.norm$M <- data$Maligno=="malignant"
# Creaci?n de variables binarias en lugar de usar la variable factor
data.norm$M <- data$Maligno=="Maligno"
# Creaci?n de variables binarias en lugar de usar la variable factor
data.norm$M <- data$diagnosis=="Maligno"
data.norm$B <- data$diagnosis=="Benigno"
View(data.norm)
# create labels for training and test data
datos.train.nn <- data.frame(data.norm[train,])
datos.test.nn  <- data.frame(data.norm[-train,])
datos.train.label <- data[train, 31]
datos.test.label <- data[-train, 31]
View(data.norm)
# simple ANN con una única neurona en la capa oculta
set.seed(params$seed.clsfier) # semilla que nos garantiza resultados reproducibles
library(neuralnet)
nn.model.1 <- neuralnet(fmla,
data = datos.train.nn,
hidden=1, linear.output=FALSE)
# create labels for training and test data
datos.train.nn <- data.frame(data.norm[train,])
datos.test.nn  <- data.frame(data.norm[-train,])
datos.train.label <- data[train, 31]
datos.test.label <- data[-train, 31]
# Creamos una formula para desarrollar el modelo
# con todas y cada una de las 30 variables:
xnam <- names(data[1:30])
(fmla <- as.formula(paste("M + B ~ ", paste(xnam, collapse= "+"))))
# simple ANN con una única neurona en la capa oculta
set.seed(params$seed.clsfier) # semilla que nos garantiza resultados reproducibles
library(neuralnet)
nn.model.1 <- neuralnet(fmla,
data = datos.train.nn,
hidden=1, linear.output=FALSE)
# Visualizamos la topología de la red neuronal:
plot(nn.model.1, rep='best')
nn.model.1.matrix <- predict(nn.model.1, datos.test[,1:32])
nn.model.1.matrix <- predict(nn.model.1, datos.test.nn[,1:32])
max.idx <- function(xxy) {
return(which(xxy == max(xxy)))
}
idx1 <- apply(nn.model.1.matrix, 1, max.idx)
prediction <- ("diagnosis")[idx1]
res <- table(prediction, data$diagnosis[-train] )
# Resultados
#require(caret)
library(caret)
(cmatrix <- confusionMatrix(res,  datos.test.nn$diagnosis, positive = "M"))
nn.model.1.matrix <- predict(nn.model.1, datos.test.nn[,1:30])
# Creamos una salida binaria múltiple a nuestra salida categórica
max.idx <- function(xxy) {
return(which(xxy == max(xxy)))
}
nn.model.1.matrix <- predict(nn.model.1, datos.test.nn[,1:30])
# Creamos una salida binaria múltiple a nuestra salida categórica
max.idx <- function(xxy) {
return(which(xxy == max(xxy)))
}
idx1 <- apply(nn.model.1.matrix, 1, max.idx)
prediction <- c('Maligno', 'Benigno')[idx1]
res <- table(prediction, data$diagnosis[-train])
# Resultados
#require(caret)
library(caret)
(cmatrix <- confusionMatrix(res,positive = "Maligno"))
nn.model.1.matrix <- predict(nn.model.1, datos.test.nn[,1:30])
# Creamos una salida binaria múltiple a nuestra salida categórica
max.idx <- function(xxy) {
return(which(xxy == max(xxy)))
}
idx1 <- apply(nn.model.1.matrix, 1, max.idx)
prediction <- c('Maligno', 'Benigno')[idx1]
res <- table(prediction, data$diagnosis[-train])
# Resultados
#require(caret)
library(caret)
(cmatrix <- confusionMatrix(res,positive = "Maligno"))
# simple ANN with only a single hidden neuron
set.seed(params$seed.clsfier) # to guarantee repeatable results
nn.model.3 <- neuralnet(fmla,
data = datos.train.nn,
hidden=3,linear.output=FALSE)
# visualize the network topology
plot(nn.model.3, rep='best')
nn.model.3.matrix <- predict(nn.model.3, datos.test.nn[,1:30])
idx1 <- apply(nn.model.3.matrix, 1, max.idx)
prediction <- c('Maligno', 'Benigno')[idx1]
res3 <- table(prediction, data$diagnosis[-train])
# Resultados
(cmatrix <- confusionMatrix(res3,positive = "Maligno"))
nn.model.3.matrix <- predict(nn.model.3, datos.test.nn[,1:30])
idx1 <- apply(nn.model.3.matrix, 1, max.idx)
prediction <- c('Maligno', 'Benigno')[idx1]
res3 <- table(prediction, data$diagnosis[-train])
# Resultados
(cmatrix3 <- confusionMatrix(res3,positive = "Maligno"))
nn.model.1.matrix <- predict(nn.model.1, datos.test.nn[,1:30])
# Creamos una salida binaria múltiple a nuestra salida categórica
max.idx <- function(xxy) {
return(which(xxy == max(xxy)))
}
idx1 <- apply(nn.model.1.matrix, 1, max.idx)
prediction <- c('Maligno', 'Benigno')[idx1]
res <- table(prediction, data$diagnosis[-train])
# Resultados
#require(caret)
library(caret)
(cmatrix1 <- confusionMatrix(res,positive = "Maligno"))
nrow(datos.train)/nrow(datos.test)
# modelo Train/test without repetition
model <- train(diagnosis ~ ., datos.train, method='nnet',
trControl= trainControl(method='none'),
# preProcess = "range",
tuneGrid= NULL, tuneLength=1 ,trace = FALSE) #
plotnet(model)
# modelo Train/test without repetition
library(caret)
model <- train(diagnosis ~ ., datos.train, method='nnet',
trControl= trainControl(method='none'),
# preProcess = "range",
tuneGrid= NULL, tuneLength=1 ,trace = FALSE) #
plotnet(model)
summary(model)
summary(model)
prediction.nnet <- predict(model, datos.test[-10])                           # predict
prediction.nnet <- predict(model, datos.test)
table(prediction.nnet, datos.test$diagnosis)
prediction <- predict(model, datos.test, type="prob")
head(prediction)
# modelo Train/test without repetition
library(caret)
library(NeuralNetTools)
model <- train(diagnosis ~ ., datos.train, method='nnet',
trControl= trainControl(method='none'),
# preProcess = "range",
tuneGrid= NULL, tuneLength=1 ,trace = FALSE) #
plotnet(model)
summary(model)
prediction.nnet <- predict(model, datos.test)                           # predict
table(prediction.nnet, datos.test$diagnosis)                                  # compare
# predict can also return the probability for each class:
prediction <- predict(model, datos.test, type="prob")
head(prediction)
# modelo 5-crossvalidation
model2 <- train(diagnosis ~ ., datos.train, method='nnet',
trControl= trainControl(method='cv', number=5),
tuneGrid= NULL, tuneLength=10 ,trace = FALSE)
plotnet(model2, alpha=0.6)
summary(model2)
prediction.nnet2 <- predict(model2, datos.test)                           # predict
table(prediction.nnet2, datos.test$Class)                                  # compare
plotnet(model2, alpha=0.6)
summary(model2)
prediction.nnet2 <- predict(model2, datos.test)                           # predict
table(prediction.nnet2, datos.test$diagnosis)                                  # compare
# predict can also return the probability for each class:
prediction2 <- predict(model2, datos.test, type="prob")
head(prediction2)
indicesT <- which(data$dignosis =='Maligno')
indicesN <- which(data$dignosis =='Benigno')
set.seed(params$seed.train)
aleatT <- sample(indicesT,15)
indicesN
indicesT <- which(data$diagnosis =='Maligno')
indicesN <- which(data$diagnosis =='Benigno')
indicesN
set.seed(params$seed.train)
aleatT <- sample(indicesT,15)
set.seed(params$seed.train)
aleatN <- sample(indicesN,15)
#10 genes aleatorios
set.seed(params$seed.train)
aleatG <- sample(1:(ncol(data)-1),10)
datasetT <- data[aleatT,aleatG]
datasetN <- data[aleatN,aleatG]
summary(datasetT)
summary(datasetN)
par(mfrow=c(1,2))
boxplot(datasetN,cex.axis=0.6,ylab='Expresi?n',main="Tejido normal",las=2)
abline(h = 500,col='red')
boxplot(datasetT,cex.axis=0.6,ylab='Expresi?n', main="Tejido tumoral", las=2)
abline(h = 500,col='red')
set.seed(params$seed.train)
aleatG <- sample(1:(ncol(data)-1),4)
#Dataset "reducidos" con las muestras
datasetT <- data[aleatT,aleatG]
datasetN <- data[aleatN,aleatG]
summary(datasetT)
summary(datasetN)
par(mfrow=c(1,2))
boxplot(datasetN,cex.axis=0.6,ylab='Expresi?n',main="Tejido normal",las=2)
abline(h = 500,col='red')
boxplot(datasetT,cex.axis=0.6,ylab='Expresi?n', main="Tejido tumoral", las=2)
abline(h = 500,col='red')
round((table(datos.train$diagnosis)["Maligno"]/ nrow(datos.train))*100,2)
library(kernlab)
install.packages("kernlab")
library(kernlab)
set.seed(params$seed.clsfier)
modeloLineal <- ksvm(diagnosis~.,data=datos.train, kernel='vanilladot')
# look at basic information about the model
modeloLineal
set.seed(params$seed.clsfier)
modeloGauss <- ksvm(diagnosis~.,data=datos.train,
kernel='rbfdot')
# look at basic information about the model
modeloGauss
#Prediccion
modGauss_pred <- predict(modeloGauss, datos.test)
#Metricas de rendimiento
library(caret)
#Modelo lineal
res.svm <- table(modGauss.pred, datos.test$diagnosis)
# look at basic information about the model
modeloGauss
#Prediccion
modGauss.pred <- predict(modeloGauss, datos.test)
#Metricas de rendimiento
library(caret)
#Modelo lineal
res.svm <- table(modGauss.pred, datos.test$diagnosis)
# Results
library(caret)
(cmatrix2 <- confusionMatrix(res.svm,positive="t"))
(cmatrix2 <- confusionMatrix(res.svm,positive="Maligno"))
library(caret)
#Particion de datos
set.seed(params$seed.train)
# We wish 75% for the trainset
inTrain <- createDataPartition(y=data$diagnosis, p=params$p.train, list=FALSE)
train.set <- data[inTrain,]
test.set  <- data[-inTrain,]
nrow(train.set)/nrow(test.set) # should be around 2 if train=2/3
# modelo solo de Train sin repeticion
set.seed(params$seed.otro)
model.svm <- train(diagnosis ~ ., train.set, method='svmLinear',
trControl= trainControl(method='none'),
# preProcess = "range",
tuneGrid= NULL, trace = FALSE) #
model.svm
prediction.svm <- predict(model.svm, test.set)                         # predict
res.svm2 <- table(prediction.svm, test.set$diagnosis)                          # compare
confusionMatrix(res.svm2, positive="Maligno")
# modelo solo de Train sin repeticion
set.seed(params$seed.otro)
model.svm2 <- train(diagnosis ~ ., datos.train, method='svmLinear',
trControl= trainControl(method='none'),
# preProcess = "range",
tuneGrid= NULL, trace = FALSE) #
model.svm2
prediction.svm2 <- predict(model.svm2, datos.test)                         # predict
res.svm3 <- table(prediction.svm2, datos.test$diagnosis)                          # compare
confusionMatrix(res.svm3, positive="Maligno")
# modelo 5-crossvalidation
set.seed(params$seed.clsfier)
model.svm4 <- train(diagnosis ~ ., train.set, method='svmLinear',
trControl= trainControl(method='cv', number=5),
tuneGrid= NULL, trace = FALSE)
model.svm4
prediction4 <- predict(model.svm4, test.set)                           # predict
res4 <- table(prediction4, test.set$diagnosis)                             # compare
confusionMatrix(res4, positive="Maligno")
#fijar la semilla para el clasificador
set.seed(params$seed.clsfier)
data.model<- C5.0(datos.train[-22], datos.train$diagnosis)
usePackage("C50")
#fijar la semilla para el clasificador
library(C50)
set.seed(params$seed.clsfier)
data.model<- C5.0(datos.train, datos.train$diagnosis)
data.model
#fijar la semilla para el clasificador
library(C50)
set.seed(params$seed.clsfier)
data.model<- C5.0(datos.train[-31], datos.train$diagnosis)
data.model
resumen<-summary(data.model)
resumen
plot(data.model)
plot(data.model, subtree=3)
data.model$dims[1]
data.predict<- predict(data.model, datos.test)
crosstb<- CrossTable(datos.test$diagnosis, data.predict,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
crosstb
cf.arbol<-confusionMatrix(data.predict, datos.test$diagnosis, positive = "Maligno")
cf.arbol
resum <- data.frame(h=NA, accu=NA, kap=NA)
resum[1,1:3] <- c("C5.0",round(cf.arbol$overall[1],3),round(cf.arbol$overall[2],3))
kable(resum[,1:3], col.names=c("modelo", "Accuracy", "Kappa"),
align= c("l","c","c"), caption = "ANáLISIS USANDO áRBOLES DE CLASIFICACIóN")
#fijar la semilla para el clasificador
set.seed(params$seed.clsfier)
model_boost10 <- C5.0(datos.train[-31], datos.train$diagnosis, trials = 10)
model_boost10
summary(model_boost10)
#fijar la semilla para el clasificador
set.seed(params$seed.clsfier)
model.boost10 <- C5.0(datos.train[-31], datos.train$diagnosis, trials = 10)
model.boost10
model.boost.pred10 <- predict(model.boost10, datos.test)
cf1<-confusionMatrix(model.boost.pred10, datos.test$diagnosis, positive = "Maligno")
cf1
